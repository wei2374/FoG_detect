classdef decision_tree    % decision_tree is a class to define a classification tree built using    % recursive binary splitting    properties        %% A decision_tree object has the following properties:        %   n : cell to store the current tree's node properties        %   tree_depth : the depth of the current tree        %   X : matrix with training data        %   max_depth : maximum depth of the tree (default = inf)        %   min_gini : the minimum gini impurity value we want at each        %           terminal node (default = 0)        %   min_size_child : the minimum number of data points in the        %           terminal nodes (default = 1)        %   min_node_properties : the minimum number of fields in a        %           terminal node (fixed at 6)        %   random_feature : boolean for random forests to only use a        %           random feature at each node        n = {};        tree_depth = 0;        X = [];        max_depth = 3;        min_gini = 0;        min_size_child = 1;        min_node_properties = 6;        random_feature = false;    end    methods        function tree = decision_tree(Data, max_depth, min_gini, min_size_child)            %% tree = decision_tree(Data, max_depth, min_gini, min_size_child)            % Method to create a decision_tree object            % Input:            %       Data : matrix of the training data            %       max_depth : value for the maximum depth of the tree            %       min_gini : value for the minimum gini impurity value in the            %       terminal nodes            %       min_child_size : value for the minimum number of data points in the            %       terminal nodes            %            % Output:            %       tree : decision_tree object                        tree.X = Data;                        % Check if the user wants to change the default values            if exist('max_depth','var')                tree.max_depth = max_depth;            end            if exist('min_gini','var')                tree.min_gini = min_gini;            end            if exist('min_size_child','var')                tree.min_size_child = min_size_child;            end        end                function tree = build_tree(tree)            %% tree = build_tree(tree)            % Method to build a binary classification tree            % Input:            %       tree : input decision tree object            %            % Output:            %       tree : built decision tree                        %% Check whether the tree has already been build, not that we            % overwrite the old tree            if ~isempty(tree.n)                disp('Decision tree is already built.')                return;            end                        %% First, we define the root node            % Get the indices, i.e., all the training data            tree.n{1}.index = 1:size(tree.X,1);            % Set the depth to 0            tree.n{1}.depth = 0;            % Set the parent node to 0            tree.n{1}.parent = 0;            % Get the probabilities of all classes            tree = tree.find_p(1);                        %% Start building the rest of the tree by recursive binary splitting            disp('Building tree...')            tree = tree.split(1);            disp('Finished building tree.')                    end                function tree = find_p(tree,n)            %% tree = find_p(tree,n)            % Method to find the class labels and probabilities of the            % different classes            % Input:            %       tree : input decision tree object            %       n : current node index            %            % Output:            %       tree : decision tree                        % Get probability of each class by finding the unique labels            % at the current node n            labels = unique(tree.X(tree.n{n}.index,end)).';            tree.n{n}.p=sum(tree.X(tree.n{n}.index,end) ...                ==labels)/length(tree.n{n}.index);                        % Set the labels at node n = the unique class labels we previously found            tree.n{n}.l = labels;                                end                function tree = split(tree,n)            %% tree = split(tree,n)            % Method to generate tree by recursive binary splitting            % Input:            %       tree : input decision tree object            %       n : current node index            %            % Output:            %       tree : decision tree                        % Find the optimal split at node n            tree = tree.find_split(n);                        % Keep track of the maximum depth of the tree            if tree.n{n}.depth > tree.tree_depth                tree.tree_depth = tree.n{n}.depth;            end                        % Check if there is any impurity after this split, if not we            % jump back out of the recursion            if length(fieldnames(tree.n{n})) < tree.min_node_properties                return            end                        %% Define the left child node            left_node_ind = length(tree.n)+1;            % Find the set of data points in the left child node            left_index = tree.X(tree.n{n}.index,...                tree.n{n}.j) < tree.n{n}.tau;            % Make sure the set is not empty            if sum(left_index)>0                %% Set the properties of the left child node                tree.n{n}.left_node = left_node_ind;                tree.n{left_node_ind}.index = ...                    tree.n{n}.index(left_index);                tree.n{left_node_ind}.depth = tree.n{n}.depth+1;                tree.n{left_node_ind}.parent = n;                tree = tree.find_p(left_node_ind);                %% Check whether the termination conditions are met                if tree.n{left_node_ind}.depth < tree.max_depth &&...                        tree.n{n}.gini > tree.min_gini && ...% min gini index                        length(left_index) > tree.min_size_child                     % Make sure the child has enough elements                    tree = tree.split(left_node_ind);                end            end                        %% Define the right child node            right_node_ind = length(tree.n)+1;            % Find the set of data points in the right child node            right_index = tree.X(tree.n{n}.index,...                tree.n{n}.j)>= tree.n{n}.tau;            % Make sure the set is not empty            if sum(right_index) > 0                %% Set the properties of the right child node                tree.n{right_node_ind}.index = ...                    tree.n{n}.index(right_index);                tree.n{n}.right_node = right_node_ind;                tree.n{right_node_ind}.depth = tree.n{n}.depth+1;                tree.n{right_node_ind}.parent = n;                tree = tree.find_p(right_node_ind);                %% Check whether the termination conditions are met                % check the maximum tree depth                if tree.n{right_node_ind}.depth < tree.max_depth  && ...                        tree.n{n}.gini > tree.min_gini && ...% min gini index                        length(left_index) > tree.min_size_child                     % Make sure the child has enough elements                    tree = tree.split(right_node_ind);                end            end        end                function tree = find_split(tree,n)            %% tree = find_split(tree,n_current)            % Method to find the optimal split at the current node            % Input:            %       tree : input decision tree object            %       n : current node index            %            % Output:            %       tree : decision tree                        % Initialise the algorithm            gini_opt = inf;            index_n = tree.n{n}.index;            X_tmp = tree.X(index_n,:);                        % Check whether there are still multiple classes in this set            if length(unique(X_tmp(:,end))) == 1                return            end                        %% Boolean property of the tree to either use all features            % of a random subset of features            if tree.random_feature                num_features = size(tree.X,2)-1;                features = unique(randi(num_features,...                    1,ceil(sqrt(num_features))));            else                features = 1:(size(tree.X,2)-1);            end                        %% Run through all features            for feature = features                % Run through all data points                for nn = 1:length(index_n)                                        % Find the impurity after the split at data point nn                    % with the current feature                    left = X_tmp(:,feature) < X_tmp(nn,feature);                    right = ~left;                    impurity = tree.staticGini_Index(X_tmp(left,:), X_tmp(right,:));                                        % If the impurity is better we overwrite the optimal values                    if impurity < gini_opt                        tau_opt = X_tmp(nn, feature);                        j_opt = feature;                        gini_opt = impurity;                    end                end                            end                        %% Write the optimal values into the current node            tree.n{n}.j = j_opt;            tree.n{n}.tau = tau_opt;            tree.n{n}.gini = gini_opt;                    end                function [error_Rate, estimations] =...                estimate_test(tree, Z_test, max_depth)            %% [error_Rate, estimations] =...            %    estimate_test(tree, Z_test, max_depth)            % Method to find the error rate and the estimations            % Input:            %       tree : input decision tree object            %       Z_test : a matrix with test data points            %       max_depth : maximum depth of the estimation            %            % Output:            %       error_rate : the error rate on the data points Z_test            %       estimations : the actual estimates of the Z_test data            %       points                        if ~exist('max_depth','var')                max_depth = inf;            end                        %% Run through all data points in Z_test and get the estimates            estimations = zeros(size(Z_test,1),1);            for i = 1:size(Z_test,1)                estimations(i) = tree.get_estimate(Z_test(i,:),max_depth).estimation;            end                        %% Find which estimates are correct            correct_indices = (estimations == Z_test(:,end));            % Calculate the error rate            error_Rate = sum(~correct_indices)/length(correct_indices);        end                function estimate = get_estimate(tree, z_in, pred_depth, next_index)            %% estimate = get_estimate(tree, z_in, pred_depth, next_index)            % Method to recursively traverse tree to get the estimate            % Input:            %       tree : input decision tree object            %       z_in : single  data points            %       pred_depth : maximum depth of the estimation            %       next_index : keep track of which node index            %            % Output:            %       estimate : the estimated class of z_in            %                        if ~exist('next_index','var')                next_index = 1;            end                        if ~exist('pred_depth','var')                pred_depth = inf;            end                        if size(z_in,1) > 1                disp('Error: Estimate one sample at at time.')                return            end                        %% Check whether the input z_in lies in the left child or right            % child            if z_in(tree.n{next_index}.j) < tree.n{next_index}.tau                                next_index = tree.n{next_index}.left_node;                                %% Check whether we have reached a terminal node or                % the max. prediction depth to break the recursion                if length(fieldnames(tree.n{next_index})) <...                        tree.min_node_properties ||...                        tree.n{next_index}.depth > pred_depth                    estimate = tree.get_estimation(next_index);                    return                else                    estimate = tree.get_estimate(z_in, pred_depth, next_index);                end            else                next_index = tree.n{next_index}.right_node;                                %% Check whether we have reached a terminal node or                % the max. prediction depth to break the recursion                if length(fieldnames(tree.n{next_index})) <...                        tree.min_node_properties ||...                        tree.n{next_index}.depth > pred_depth                    estimate = tree.get_estimation(next_index);                                        return                else                    estimate = tree.get_estimate(z_in, pred_depth, next_index);                end                            end                    end                function out = get_estimation(tree,index)            %% out = get_estimation(tree,index)            % Method to get the estimation properties at current node            % Input:            %       tree : input decision tree object            %       index : index of node at which the estimation should            %       take place            %            % Output:            %       out : a struct of the estimation data            %                        %% Get the probabilities and classes at node index            prob = tree.n{index}.p;            classes = tree.n{index}.l;                        out.probability = prob;            out.classes = classes;            [~,ind] = max(prob);            % Make a hard decision            out.estimation = classes(ind);        end                function plot_tree(tree, print_Labels)            %% plot_tree(tree, print_labels)            % Method to plot the classification tree            % Input:            %       tree : input decision tree object            %       print_Labels : boolean whether or not to print the node            %       properties            %                        num_Nodes = length(tree.n);            parents = zeros(1,num_Nodes);                        if ~exist('print_Labels','var')                print_Labels = true;            end                        %% Get the parents of all the nodes            for i = 1:num_Nodes                parents(i) = tree.n{i}.parent;            end                        %% Plot the tree at the right heights            tree.treeplot_algined(parents,print_Labels);                    end                function treeplot_algined(tree, p, print_Labels)            %% treeplot_algined(tree, p, print_labels)            % Method to plot the classification tree            % Input:            %       tree : input decision tree object            %       p : vector of parent indices            %                        %% Plot tree with nodes aligned at the correct depth            [x,y,~]=treelayout(p);                        leaves = find( y == min(y) );            nodes = find( y ~= min(y));            num_layers = 1/min(y)-1;            chains = zeros(num_layers, length(leaves));            for l=1:length(leaves)                index = leaves(l);                chain = [];                chain(1) = index;                parent_index = p(index);                j = 2;                while (parent_index ~= 0)                    chain(j) = parent_index;                    parent_index = p(parent_index);                    j = j+1;                end                chains(:,l) = padarray(flip(chain),...                    [0, num_layers-length(chain)], 'post');            end                        y_new = zeros(size(y));            for i=1:length(p)                [r,~] = find(chains==i, 1);                y_new(i) = max(y) - (r-1)*1/(num_layers+1);            end                        figure;            plot(x, y_new, 'o');            hold on            for c=1:size(chains, 2)                line_x = x(chains(chains(:,c)>0, c));                line_y = y_new(chains(chains(:,c)>0, c));                line(line_x, line_y);            end            title(['Decision Tree for Min. Gini: ',...                num2str(tree.min_gini),', Max. Depth: ',...                num2str(tree.max_depth),', Min. Child Size: ',...                num2str(tree.min_size_child)])            %% If the print is true print the internal node decision rule            % and terminal node estimation            if print_Labels                y_shift = 0.02;                for i=leaves                    text(x(i),y_new(i)-y_shift,num2str(tree.n{i}.l),...                        'HorizontalAlignment','center')                end                for i=nodes                    str = ['z_',num2str(tree.n{i}.j),'>=',...                        num2str(tree.n{i}.tau)];                    text(x(i)-y_shift,y_new(i)-y_shift,str,...                        'HorizontalAlignment','left')                end            end                    end                function plot_3D_data(tree)            %% plot_3D_data(tree)            % Method to plot input 3D data            % Input:            %       tree : input decision tree object            %            %            % Only plot if the input data is of the correct dimension            if size(tree.X,2) == 4                figure;                hold on;                labels = unique(tree.X(:,end));                str = cell(length(labels),1);                for i = 1:length(labels)                    index = tree.X(:,end) == labels(i);                    scatter3(tree.X(index,1), tree.X(index,2), tree.X(index,3))                    str{i}= ['\theta_',num2str(i-1), ' = ', num2str(labels(i))];                end                legend(str)                grid on                xlabel('z_1')                ylabel('z_2')                zlabel('z_3')                title('Training Data')                view([-25,30])                rotate3d on;            end        end                function plot_3D_surfaces(tree)            %% plot_3D_surfaces(tree)            % Method to plot input 3D data            % Input:            %       tree : input decision tree object            %                        % Only plot if the input data is 3D            if size(tree.X,2) == 4                col = [0.4660    0.6740    0.1880];                %% Plot the decision boundaries of the first three layers                for depth = 1:3                    figure;                    for sub = 1:(2^(depth-1))                        if depth ~= 1                            subplot(1,(2^(depth-1)),sub)                            node = children(sub);                        else                            node = 1;                        end                                                hold on;                        labels = unique(tree.X(tree.n{node}.index,end));                        str = cell(length(labels),1);                        for i = 1:length(labels)                            index = tree.X(tree.n{node}.index,end) == labels(i);                            scatter3(tree.X(tree.n{node}.index(index),1),...                                tree.X(tree.n{node}.index(index),2),...                                tree.X(tree.n{node}.index(index),3))                            str{i}= ['\theta_',num2str(i), ' = ', num2str(labels(i))];                        end                        %% Define the mesh surface at the decision boundary                        switch tree.n{node}.j                            case 1                                ylim = linspace(min(tree.X(tree.n{node}.index,2)),...                                    max(tree.X(tree.n{node}.index,2)),...                                    21);                                zlim = linspace(min(tree.X(tree.n{1}.index,3)),...                                    max(tree.X(tree.n{1}.index,3)),...                                    21);                                                                [Y_tmp,Z_tmp] = meshgrid(ylim,zlim);                                mesh(tree.n{node}.tau*ones(21,1), Y_tmp,Z_tmp,...                                    'LineWidth',1,'EdgeColor',col,'FaceAlpha',...                                    .5,'FaceColor',col);                            case 2                                xlim = linspace(min(tree.X(tree.n{node}.index,1)),...                                    max(tree.X(tree.n{node}.index,1)),...                                    21);                                zlim = linspace(min(tree.X(tree.n{node}.index,3)),...                                    max(tree.X(tree.n{1}.index,3)),...                                    21);                                                                [X_tmp,Z_tmp] = meshgrid(xlim, zlim);                                mesh(X_tmp, tree.n{node}.tau*ones(21,1),Z_tmp,...                                    'LineWidth',1,'EdgeColor',col,'FaceAlpha',...                                    .5,'FaceColor',col);                            case 3                                xlim = linspace(min(tree.X(tree.n{node}.index,1)),...                                    max(tree.X(tree.n{node}.index,1)),...                                    21);                                ylim = linspace(min(tree.X(tree.n{node}.index,2)),...                                    max(tree.X(tree.n{node}.index,2)),...                                    21);                                [X_tmp,Y_tmp] = meshgrid(xlim,ylim);                                mesh(X_tmp,Y_tmp, tree.n{node}.tau*ones(size(X_tmp,1),size(Y_tmp,1)),...                                    'LineWidth',1,'EdgeColor',col,'FaceAlpha',...                                    .5,'FaceColor',col);                        end                        legend(str)                        grid on                        xlabel('z_1')                        ylabel('z_2')                        zlabel('z_3')                        title(['Split at depth: ', num2str(depth-1),...                            ' for z_',num2str(tree.n{node}.j), ' < ',...                            num2str(tree.n{node}.tau)])                        view([-30,30])                        if depth == 1                            limits = [floor(min(tree.X(:,1:end-1))), ceil(max(tree.X(:,1:end-1)))];                            limits = limits([1,4,2,5,3,6]);                        end                        axis(limits)                        rotate3d on;                                            end                                        if depth ~= 1                        ch = [tree.n{children}];                    else                        ch = tree.n{node};                    end                    children = [ch.left_node, ch.right_node];                    if depth == 2                        children = children([1,3,2,4]);                    end                end            end                    end                function plot_errors(tree, Z_test)            %% plot_errors(tree, Z_test)            % Method to plot input the estimation error rates for the            % training data and some test data stored in Z_test            % Input:            %       tree : input decision tree object            %       Z_test : matrix of test data            %                        %% Get the error rates for different depths of the tree            test_error = zeros(tree.tree_depth,1);            train_error = test_error;            for i = 1:(tree.tree_depth)                train_error(i) = tree.estimate_test(tree.X,i);                test_error(i) = tree.estimate_test(Z_test,i);            end                        % Plot the error rates against tree depth            figure;            plot(1:tree.tree_depth, test_error)            hold on;            plot(1:tree.tree_depth, train_error)            legend('Test error','Training error')            xlabel('Tree Depth')            ylabel('Error Rate')            title(['Errors vs. Tree Depth for Min. Gini: ',...                num2str(tree.min_gini),', Max. Depth: ',...                num2str(tree.max_depth),', Min. Child Size: ',...                num2str(tree.min_size_child)])            grid on;        end    end    methods (Static)        function gini = staticGini_Index(Z_left, Z_right)            %% gini = staticGini_Index(Z_left, Z_right)            % Method to get the gini impurity value at the current split            % into a left and right set            % Input:            %       Z_left : set of data in the left child            %       Z_right : set of data in the right child            %            % Output:            %       gini : the impurity value            %                        %% Get the total number of data points the the current sets            M_total = size(Z_left,1) + size(Z_right,1);                        %% Get the number of data points in the left child            if ~isempty(Z_left)                M_childl = size(Z_left,1);            else                M_childl = 1;            end                        %% Get the number of data points in the right child            if ~isempty(Z_right)                M_childr = size(Z_right,1);            else                M_childr = 1;            end                        y_labels = unique([Z_left(:,end); Z_right(:,end)]).';                        %% Calculate the gini impurity value using            % eq. (4.7) from the lecture notes            gini = 0;            for y = y_labels                M_kl = sum(Z_left(:,end) == y);                M_kr = sum(Z_right(:,end) == y);                p_kl = 1- M_kl / M_childl;                p_kr = 1- M_kr / M_childr;                gini = gini + p_kl*M_kl + p_kr*M_kr;                            end                        gini = (1/M_total) * gini;                    end    endend